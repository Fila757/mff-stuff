https://mj.ucw.cz/vyuka/dsnotes/

Vyhledávací stromy ((a,b)-stromy, Splay stromy)
- BVS: 
    - každý uzel nanejvýš dva potomky
    - každý uzel má klíč, podle něj uspořádány
    - levý podstrom uzlu obsahuje klíče menší než uzel, pravý větší 

    - Nemodifikující: Find, MIN, MAX, Prec, Succ
    - Modifikující: Insert, delete 
    
    - Delete 
        - Pokud dva potomky, najdu max v levém/min v pravém podstromě, vyměním (prec X) a smažu
    - Strom může zdegenerovat, třeba vyvažovat -> O(log(n)) hloubka 

- (a, b) stromy 
    - a >= 2, b >= 2a-1
    - každý vnitřní vrchol má alespon [a, b] synů
    - kořen má [2, b] synů
    - všechny cesty z kořene do listu jsou stejně dlouhé 

    - hodnoty (většinou) pouze v listech, vnitřní vrcholy jen ukazatele na lower nodes 
    - pro (a, b) strom o hloubce h platí: a2^h-1 < #leaves < b^h
        - různé hodnoty a,b: (2, 3), velikost cache, disk bloku, ... 

    - FIND(x): prohledej od začátku vrať 
    - INSERT(x): najdi vrchol v pod který patří x, 
        - pokud má <b potomků, přidej hotovo 
        - pokud má b potomků, rozštěp v, vytvoř nové v' pro prvních b/2+1 dětí, vlož do parent(v) rekurzivně
    - Delete(x): najdi vrchol v pod kterým je x
        - pokud má >a potomků, smaž 
        - pokud má v společné s levým/pravým sousedem >2a potomků, přesuň jednoho potomka zleva/zprava (musí se přerotovat přes otce/updatovat v otci, aby byla zachována podmínka vyhledávacích stromů)
        - otherwise: smaž x, spojen se s levým/pravým sousedem (smaž v otci, rekurzivně dál)
    - ORD(i): vrátí i-tý nejmenší prvek, O(log(n)) pokud si udržuju počty v podstromech v každém nodu

    - paralelní verze: 
        - b>=2a 
        - při insert a delete provádím změny už cestou dolů (štěpím dolů při insert u b potomků, slučuju při delete a)
        - mohu rovnou odemykat navštívené vrcholy, jistota že se nebudu muset vracet 
    
    - analýza: 
        - počet štěpení / slučování vrcholů při "m" insertech, "l" deletech je O(m + l + log(n)), amortizovaně O(1) štěpení/sloučení za operaci
            - ~v podstatě dynamické pole kind of důkaz TODO 
    
    - a-sort 
        - z prvků nedříve sestrojí a,b strom 
        - při vkládání hledáme pozici pro aktuální klíč od poslední pozice, počet kroků: 
            - menší: nahoru (a analogicky dolů) max log_2(|{j | j < i; xi+1 < xj}|)
            - větší: nahoru (a analogicky dolů) max log_2(|{j | j < i; x < xj}|)
        - předpoklad: štěpení amortizovaně O(1)
        -> O(n*log(F/n)), F je počet inverzí 

    - b-stromy: disky/databáze, b velké, a=b/2
        - b+: data pouze v listech, sousední listy spojeny ukazateli 
        - b*: plnější vrcholy, dolní limit není 1/2 maxima, ale třeba 2/3 maxima, při zaplnění se sdílí klíče se sousedem, až u zaplnění obou se rozpadnou ze 3 na 2

    - červeno-černé: v podstatě odpovídají 2, 4 stromům, implementačně BVS 
        - listy černé, červené vrcholy pouze syny černých vrcholů, všechny cesty z x do listu stejný počet černých vrcholů 
        - vyvažování rotacemi 
        - One way to see this equivalence is to "move up" the red nodes in a graphical representation of the red–black tree, so that they align horizontally with their parent black node, by creating together a horizontal cluster. In the B-tree, or in the modified graphical representation of the red–black tree, all leaf nodes are at the same depth.

- splay tree 
    - adaptivní binární vyhledávací strom, udržuje často používané prvky blízko u kořene 
    - cena za operaci až O(n), amortizovaně za k operací O(klogn + nlogn)

    - splay operace: přesuneme prvek x do kořene pomocí násl. rotací 
        - zig-zag: LL -> RR 
        - zig-zag: LR -> balanced tree 
        - zig: L -> R 

        - pokud to lze používají se dvojrotace

    - FIND: normální BVS + splay, pokud neexistuje Splay na posledním touched
    - Insert: normální BVS + splay 
        - zvýší potenciál, zvýšený rank odhadnu rankem vrcholu výše -> teleskopicky -> jen o log(n)
    - delete: splay do kořene, delete, splay maximum v left (max. v kořeni, nemá pravého syna), připojíme R pod L doprava

    - analýza: 
        - potenciálová metoda: t = a + p(T') - p(T) | a: skutečný čas operace, p(T): potenciál stromu 
            - p(T): sum_x€T(r(x)) | r(x): log(2(s(x))) | s(x): počet vrcholů pod x 
            - r(x): rank uzlu, maximálně log(n), tj. p(T) maximalne n*log(n), čím nižší tím vyváženější 

            - amortizovaný čas m operací: t1 + t2 + t3 ... + tm = sum_i..m a_i + p(Ti) - p(Ti-1) = 
                                                                = sum_i..m a_i + p(Tm) - p(T0)
            - reálný čas se od amortizováného liší maximálně o p(Tm) - p(T0), tj. o n*log(n)
        
        - operace splay(x) zabere amertizovaný čas <= 3(r'(x) - r(x)) + 1 // +1 pouze u zig a to jen u kořene, v splay nastane jen jednou 
            - při rotacích se mění pouze rank x, parent, grandparent 
            - skrze odhad na změnu r(x) máme amortizovanou cenu operace splay: O(log(n))
            - celková složitost je tedy O(mlogn + nlogn): amortizované splay + maximální změna potenciálu

            - +1 jenom u kořene je důležitá, jinak se teleskopicky neposčítá -> vyšlo by to at least lineárně s hloubkou stromu -> problém 


    - v optimálním binárním vyhledávajícím stromě, kde k prvku xi přistupujeme s pi je očekávaná cena úspěšné vyhl. operace ~ entropie pi rozdělení
        - splay trees jsou jen o konstantu horší než optimální statické stromy (static optimality)
        - hypotéza: to samé platí i o dynamických strukturách (splay je dynamicky optimální)

        - dokazuje se přes váhy

    - working set theorem: pokud provedeme nlogn úspěšných vyhl. operací -> amortizovaná cena jedné O(1+logt), t: počet operací od předchozího přístupu k prvku 
    - sequential access theorem: pokud vyhledáme všech n prvků ve vzestupném pořadí: O(1)

- intervalové dotazy 
    - triviálně v setříděném poli: binárně najdu hrany O(logn + p), enumerate mezi nimi (p)
    - pro BVS: interval(v) :: všechny hodnoty jejiž vyhledání navštíví v: ~všichni potomci + v, hrany nekonečno, ...
        - chce si to pamatovat interval(v) v každém vrcholu pro efektivní rangeQuery (O(nlogn) stavba)
        - snadná aktualizace při rotaci z přímých potomků

    - rangeQuery(v, Q):
        - iterativně prochází po hranách / reportne celý podstrom 
        - maximálně dvě hrany O(log(n)) pro vyvážený BVS 

    - díky BVS .next() -> můžeme také enumerovat 
        - next je O(h+k) :: https://stackoverflow.com/questions/40561235/time-complexity-of-finding-k-successors-in-bst (remember: values are on every level)


Haldy (regulární, binomiální).
- stromová strkutura splňující, že potomci jsou vždy větší (minimální) než rodič 
- Dolů: O(d*logd(n)), nahoru O(logd(n))
- k-regulární halda 
    - každý vrchol má k synů 
    - pro 2: snadno reprezentovatelné v poli 
    
    - heapify od zdola je O(n): 
        - nandám do pole, od posledních prvků upravuju skrze bubble down 
        - počet bublání: sum_i:0..h 2^(h-i) -> 2^h * sum_i:0..h i/2^i -> 2^h*C[neb konverguje] -> O(n) vůči počtu prvků 
        - většina prvků nemusí migrovat z dolních (největších) pater 

    - Insert: přidám na konec, heap-up 
    - Delete-min: smažu vrchol, na jeho místo dám poslední prvek, heap-down 
    

- binomiální halda: 
    - rychlejší insert (líná)
    - snadný/rychlý (logn) merge dvou hald 

    - halda je množina více haldových stromů, každý velikost 2^k pro nějaké k, ve spojovém seznamu
    - pamatujeme si, kde leží nejmenší prvek 

    - binomiální strom
        - rekurzivní: pokud je řád k==0 -> T jenom kořen
        - T má právě k synů, tito synové jsou kořeny podstromů 0...k-1

        - alternativně: bk = bk-1 + pod kořenem další bk-1
        - 2^k vrcholů v k hladinách 

- zbrklá bin-halda
    - insert: O(logn), amortizovaně za určitých podmínek O(1), vložení n prvků za sebou je O(n)
        - přidám nový haldový strom velikosti 1, opravím strukturu, projdu stromy a když dva stejné velikosti, tak spojím ~ binární sčítání 
        - slití stromů velikosti 2^i proběhne práve n/2^i krát 
    - min: O(1), easy 
    - delete-min: O(logn)
        - smaže kořen stromu s minimálním prvkem, zbytek stromu rozpadneme, fixup jako u insertu 
    - increase: bublani dolu by trvalo log^2n, protože až log(n) synů 

- liná varianta: 
    - insert: O(1) přidám nový strom, neupravuju, fixnu pointer na nejmenší prvek 
    - delete-min: O(n), O(logn) amortizovaně
        - smažu kořen stromu s min prvkem 
        - opravím haldu (vytvořím pole délky log(n)), prvky (stromečky) postupně insertuju, při kolizi sloučím 

    - analýza: 
        - T: C * |T| // konstanta * počet stromů v haldě
        - Insert: C + T' - T = C + 1 = O(1) // přibyl jeden strom
        - Min: C + 0 = O(1) // nic se nezměnilo
        - Delete-min: C * |T| + T' - T = C * |T| + o(logn) - C * |T| = O(logn) // počet stromů po sloučení max logn

- fibonacciho halda 
    - chceme přidat operaci decrease key (např. pro dijkstru), chceme O(1) namísto O(logn)
    - jinak binomiální halda 

    - každý vrchol si udržuje své potomky ve spojovém seznamu, jejich počet, zda o nějakého už přišel (označení)
    - extract: stejně jako u líné, řád: počet synů vrcholu (pamatuje si každej vrchol)
    - decrease key: 
        - snížím hodnotu, pokud haldovitost ok, nic jinak
        - odtrhni podstrom v, zařaď do seznamu stromů 
            - pokud byl označen, odznač 
            - pokud rodič není označen, označ jej 
            - pokud rodič je označen (již o nějakého potomka přišel), rekurzivně otrhni rodiče 
        -> každý rodič může přijít o max jednoho potomka než je odtržen, vyjma kořene

        - časová složitost O(1) amortizovaně (worst case může nastat kaskáda)
            - skrze potenciál: C * |T| + 2 * #označených vrcholů
            - Dec-key: O(1)
            
                - pokud provede k řezů: 
                    - odznačí k vrcholů -2Ck 
                    - přidá k stromů do haldy: + Ck 
                    - označí jeden nový: +2c 

                -> C*k - 2C*k + C*k + 2c = O(1)

            - delete min: O(logn) 
                - řád stromu: počet potomků kořene 
                - na rozdíl od binom nemusí být 2^k, má ale nejméně Fk+2 vrcholů -> exp. 

    - existují i varianty, které garantují worst case always operace, velké konstanty 
    - pairing heap: podobně amort, rychlejší v praxi: odtran strom zakoreneny v danem prvku, dec, merge in zbytku haldy odstranene stromky 

Hašování, řešení kolizí, univerzální hašování, výběr hašovací funkce
- motivace: slovníkový problém : chceme reprezentovat množinu z nějakého universa 
- operace:
    - member 
    - insert 
    - delete 

- hashovací funkce: h: U -> [m], tabulkou o velikosti m 
    - kolize x != y && h(x) == h(y)

- notace: 
    - n = |S| | množina prvků k uložení 
    - N = |U| | universum 
    - m: velikost hashovaci tabulky  
    - a: n/m faktor zaplnění 

- řešení kolizí: řetězení ve spojových seznamech
    - předpoklady
        - hashovací funkce h rozděluje data rovnoměrně 
        - množina S je náhodný výběr U s náhodným rozdělením 

    - očekávaná délka řetězců: P(l(i) = l) = n over l * 1/m^l * (1-1/m)^n-l 
        - aproximace pro nekonečné universum a seznamů 
    -> střední hodnota je n/m, rozptyl n/m*(1-1/m)
    -> očekávaná délka nejdelšího log(n)/loglog(n) ??? where is m ???

    - očekávaný počet testů: 
        - úspěšné: 1 + a/2 (1 + polovina délky)
        - neúspěšné: odpovídá délce průměrného řetězce + situace když je prázdný: e^-a + a
            - e^-a :: (1 - 1/m)^n :: šance, že je řetězec prázdný 

    - nevýhody: 
        - spojové seznamy jsou cache insensitive, v reálu ne úplně rychlé 
        - nutné alokovat dodatečnou paměť, také problém 

- řešení kolizí: přemístování/přesuny 
    - zavedeme do hashovací tabulky pomocné ukazatele, vše ukládáme do ní 
    - dopředné a zpětné odkazy v rámci stringu prvků 

    - insert: pokud už na místě je jiný prvek, přehodíme ho na další volné místo, vložíme aktuální na jeho místo, upravím pre/pos pointery
        - a, b -> 1, jdou na 1, 2
        - c -> 2, přesune b na 3, aktualizuje a-next na 3, dá c na 2 
    - delete: při odstranění prvního prvku na jeho místo musíme přesunout druhý, ... 
    - stejná analýza jako u spojových seznamů delete/insert pomalejší: copy vs test 

- hashování se dvěma ukazateli 
    - podobné jako ^^, ale pouze namísto next/previous jen next+begin 
    - rětězec nemusí začínat na své pozici, (c se dá rovnou na 3, na 2 se dá begin pointer na 3.)

    - počet testů o něco vyšší 
        - úspěšné: 1 + a^2/6 + a/2
        - neúspěšné: řádově podobně

- srůstající hash: klasický linear probing
    - Insert: pokusím se vložit do prvku, případně na první volnou pozici za řetězcem co h(x) prochází
    - Find: jde od h(x) a testuje do konce řetízku 
    
    - vytváří velké clustery 
    - cache friendly 

    - exp number of probes: 
        - for: m >= (1+e) * n
        - O(1/e^2) ~ O((n/m-n)^2) ~ O(1) for <1 alpha: úplně random funkce / tabulation
        - omega(sqrt(n)): pro 2 nezávislé 

    - problém s delete pokud není "next" pointer jako níže 

- srůstající hash: 
    - pouze next (???proč next???), vkládám do libov. řetězce co h(x) prochází 
    - verze s pomocnou/nepomocnou pamětí 

    - velmi friendly ke cachím, linear probing v podstatě 

    - problém s operací DELETE 
        - možné jen označit prvek jako deleted, zpomaluje vyhledávání 
    - na konec řetězce (LICH), za první prvek (EICH), ...

    - s pomocnou pamětí 
        - dvě pamětí:
            - přímo adresovatelná hash funkcí 
            - pomocná část 

        - při kolizích nejprve ukládáme do sklepa, teprve pak do adresovatelné, oddalujeme srůstání řetězců
            - do určitého momentu podobné separovaným 
            - do sklýpku ukládáme/adresujeme podobným způsobem 

    - basic verze LICH 
        - O(1/e^2) pro úplně random hash funkci / tabulation
        - Omega(sqrt(n)) pro 1, 2 nezávislou hash funkci 
        - 

- dvojité hashování 
    - h1, h2, musí být nezávislé: h1(x) == h1(y) => h2(x) != h2(y)
    - při insertu hledám nejmenší i: ab h1(x) + i*h2(x) mod m bylo volné 

    - je nutné, aby i*h2(x) mod m měl prosté posloupnosti (z každého políčka řetěz po celé tabulce)
    - lineární přidávání ~ "h2(x) = 1 always" 

    - pro analýzu předpokládáme, že h2 tvoří náhodné permutace zkoušených memory buněk -> není přesné 
    - analýza: 
        - neúspěšný: ~1/(1-a)
        - úspěšný: 1/a * ln(1/(1-a))

- použití
    - lineární funguje dobře pro tabulku zaplněnou do 70%, dvojité až do 90%.
    - posouvající: musím posouvat + cache 

    - implementační detaily: 
        - pro hledání volných řádků: stack 
        - přeplnění se řeší přehashováním (větší tabulka + nová hash funkce) při určitém zaplnění 

- kukaččí hashování: 
    - používáme dvě funkce, h1, h2, prvek musí být buď v h1(x), nebo h2(x), v každé příhrádce jen jeden 
        - funkce z 6logn nezávislého systému, 6-nezávislá funkce nestačí, tabulation hashing stačí i když není ani 4-nezávislé 
    - Find/delete triviálně O(1)
    - insert: 
        - pokud jedna z lokací volných, vlož
        - pos <- h1(x)
        - 6log(n)-krát se pokus: 
            - pokud pos empty, vlož na pos, return 
            - swap x, pos // ulož na pozici, pokus se přehashovat prvek, co tam byl původně

            - if pos == h1(x): pos = h2 else pos = h1 // zkus vymemeny prvek udat na druhou pozici než kde je 
        - přehashuj celou tabulku s novými h1, h2, případně zvětš tabulku, vlož
    - analýza:
        - vrcholy jsou přihrádky, hrany jsou dvojice (h1(x), h2(x))
        - p spojení dvou vrcholů i, j cestou délky k je < 1 / mc^k, m < 2cn 
        - p cyklu max 1/2
        -> p přehashování: 1/n^2, způsobí n insertů každý log(n) -> O(1)

        - amortizace insert bez rehash je O(1)
        - amortizace insert s rehash je O(1)


- perfektní hashování: 
    - šance na perfektní hashovací funkci na první dobrou: malá 
        -> kolize v sekundárních tabulkách kvadratické velikosti
        - sekundární m > cn^2 -> p[není perfektní] < 1/2 -> E[počet pokusů] == 2
    - vím, že bude stačit O(n) paměti celkově 
        - primární tabulka: O(n)
        - sekundární celkem také O(n) (velikost sekundárních vychází z c-universálnosti použitých hash. funkcí, kvadraticky kolizí -> O(n))

- hashovaci funkce/universalnost: 
    - motivace: pro N > mn, pro každou hash funkci h existuje S y U velikosti n, že h hašuje všechny prvky S do jedné příhrádky

    - systém H je c-universální: Vx,y€U, x!=y, P_h€H[h(x) == h(y)] <= c/m 
        - pokud vybereme h z H náhodně, šance kolize x a y je menší než c * absolutneNahodnaFunkce 
    - H je k,c-nezávislý: x1, x2, ... xk€U různá, a1...ak€P: Pr_h€H[AND h(xi) == ai] <= c/(m^k)
        - když vybereme h náhodně, šance, že given items jsou mapped to given buckets je nejhůř c*absolutneNahodnaFunkce

    - H je 2, c nezávislý => H je c universální 
        - 1 nezávislost je extrémně slabá, splněná např. konstantou 
    - c-universální => E_h€H[#x, h(x) == h(y)] <= c*n/m
        - expected počet kolizí s y je <= c*n/m

    - skládání funkcí: Nechť H je 2, c nezávislý systém funkcí ... potom H* {h mod m | h z H} je 2c universalni a 2, 4c nezávislý 

- výběr hashovací funkce:
    - https://drive.google.com/drive/folders/1GDNVCKzAfSc3aSd3rieyeWwIbmzlVhig
    - úplně náhodná: nepraktické, třeba omega(U*logm bitů paměti) // pro každý si pamatuju target 
    - chceme: 
        - efektivně náhodně vybrat h z H :: O(1)
        - rychle vyhodnotit h(x) :: O(1)

    - mod: nechť p >= |U| je prvočíslo, pak H = {h_ab | a, b z [p], a!=0} kde h_ab(x) = ax + b mod p mod m 
        - je 1-universální 
            - důkaz: r = ax + b mod p || s = ay + b mod p, soustava 2 nezávislých lineárních rovnic, E právě jedno řešení pro konkrétní r, s 
                -> bijekce mezi a, b a r, s :: obojí z p^2 
            -  započítáme modulo m -> odhad špatných dvojic a, b pro které h_ab(x) == h_ab(y)
                - r, s kde r =(mod m)= s 
                - pro každé r, kolik je s: p-1/m
                -> celkem dvojic: p*(p-1) -> Pr[dvojice špatná] <= 1/m
        - je 2, 4 nezávislý 
            - Pr_r,s[r==i and s==j] <= 4/m^2 ..nezávislost..Pr_r[r==i] <= 2/m 
            - r kongruentních s i nejvýše ceil(p-m) <= (p+m-1)/m <= 2p/m
        - není 3 nezávislý 
            - zvolme x, y, z že x + y = 2z, m == p 
            - chceme aby platilo ax + b == i and ay + b == j and az + b == k, kdydkoliv 2k == i + j -> pravdepodobnost vyjde 1/m^2, není <= 1/m^3

    - c universalita je daleko od skutečné náhodnosti
        - viz absence 3-nezávyslost c-universálních  

    - polynomy: 
        - Pk = {ha | a z Zpk}, ha(x) = sum ai*x^i mod p 
        - je k, 1 nezávislý 
            - danými body (xi, h(xi)) prochází právě jeden polynom stupně menšího než d, možných polynomů je m^k
        - O(k) výběr a vyhodnocení 

    - multiply shift: 
        - 2^32 -> 2^l: x*a (odd number), truncate to 32 bits, move to extract horních l bitů 
        - 2 universální, NE 2 independent, protože ha(0) = 0  (stačí přidat random bias)

    - tabulation hashing 
        - t tabulek o 2^k entries -> 2^l, xor jednotlivé results 
        - 3-nezávislé, ne 4 nezávislé, i přesto určité velmi dobré vlastnosti (cuckoo kde stačí namísto >> 6-nezávislé)
        - O(t) eval, O(t*2^k) init 

    - hashování vektorů 
        - skalární součin s a ze Zpd 
        - s biasem (a, b) 2, 2-nezávislé, jinak 1-universální 
        - rolling hash: ha(x) = SUM i..d-1 of xi+1 * a^i 
            - d-universální 
            - přehashovat v string searchi rychle: Hj+1 = aHj - xj*a^d + x(j+d)

Analýza nejhoršího, amortizovaného a očekávaného chování datových struktur
- uvažujme dva zdroje: paměť, čas 
    - formálně: počet bitů, počet instrukcí RAMu / přechodů TS
    - neformálně: počet elementárních entit, např. měst pro TSP, ..., počet elementárních časových jednotek
        - předpokládáme omezenost velikosti čísel / entit (větší diskuze viz složitost a vyčíslitelnost: číselné problémy / NP úplnost)
        - předpokládáme konstantnost elementárních operací (vychází z maximální velikosti entit (čísel) výše)
        - všechny mezivýsledky pol(vstup, max_cislo_ze_zadani)

    - složitost: funkce f, která udává počet kroků/paměti algoritmus potřebuje vůči velikosti vstupu "n" 

- asymptotická notace: // možná bych se měl naučit řeckou abecedu ¯\_(ツ)_/¯ 
    - f(n) z O(g(n)): Ec>0 En0>0 Vn>n0 0 <= f(n) <= c * g(n)        // menší či rovno 
    - f(n) z Omega(g(n)): Ec>0 En0>0 Vn>n0 0 <= f(n) >= c * g(n)    // větší či rovno 
    - f(n) z Theta(g(n)): f z O(g(n)) and f z Omega(g(n))           // asymptoticky stejná 
    - f(n) z o(g(n)): Vc>0 En0>0 Vn>n0 0 <= f(n) < c * g(n) // ostře menší  
    - f(n) z w(g(n)): Vc>0 En0>0 Vn>n0 0 <= f(n) > c * g(n) // ostře větší 


- různé analyzy
    - v nejlepším případě: většinou není moc užitečná, může pomoci s odhadem spodní hranice možného zlepšení 
    - v nejhorším případě: garantuje nám absenci překvapení (ale v reálném životě pozor na konstranty!)
        - občas nelze snadno určit nejhorší možný vstup: uvažujeme nejhorší průchod aniž by bylo jisté, zda k němu existuje vstup (better safe than sorry)
        - mnohdy může dobrá implementace vést k minimalizaci pravděpodobnosti nejhoršího případu (hašovací tabulky)
    - analýza v průměrném případě: Expected value přes všechny možné vstupy / všechny možné nedeterministické průchody
        - mnoho problémů má worst case vcelku nepravděpodobný (např. quicksort, hashování) a v reálu je průměr relevantnější než worst case
        - stejně jako amortizovaná: pozor na realtime/soft realtime systémy

- amortizovaná složitost: 
    - cena nejhoršího (nebo průměrného) případu pro sekvenci operací 
        - pro interaktivní aplikace / realtime systémy: může být problém 
    - můžeme dostat těsnější odhad na cenu jedné operace pokud se v rámci nich děje nějaký větší book-keeping (nejhorší případ uklízí za častější dobré případy)

    - 3 techniky analýzy
        - agregovaná analýza: dívám se na sérii operací přímo 
        - účetní metoda: při každé operaci něco zaplatíme, platbu pak používáme na zaplacení operací book-keepingu
        - potenciálová metoda: definujeme vhodnou nezápornou potenciálovou funkci, která charakterizuje uklizenost 
            - amorT = realT + pN - p0 | pN: potenciál po, p0: potenciál před 
            -> realT = amorT - (pN - p0)
            -> realT = amorT + p0 - pN 
            - p0 <= pN: pokud neplatí, musíme započítat inicializaci na začátku abychom nedlužili čas (jinak bychom na začátku nadefinovali obří potenciál, všechno z něj free)


- analýza: binární čítač 
    - agregovaná: 
        - poslední bit se změní při každé operaci: k-krát 
        - předposlední při každé druhé: k/2
        - i-tý od konce: k/2^i 
        -> sum_i..n: ceil(k/2^i) <= sum_i..n 1+k/2^i <= n+2k
    - účetní metoda: 
        - změna jednoho bitu stojí jeden žeton, na každou operaci dostaneme dva žetony 
        - u každého jedničkového bitu v sekvenci si uchováme jeden žeton, v každém okamžitku na jedničkách žetony
        - při incrementu: vynulování máme předplaceno 
        - každá operace stojí 2 žetony, prvních j-nulování se zaplatí s uložených žetonů, pak jeden žeton za 0->1, a jeden žeton na uložení 
    - potenciálová metoda: 
        - p(x) počet jedničkových bitů 
        - j: je počet jedniček vynulovaných při operaci  -> 1-j: zmena potenciálu
        - amorT: j + 1 + (1 - j) = 2
        - celkový čas tedy: T = SUM_i..k realT = k * amorT + p0 - pk <= 2k + n 
- analýza: dynamické pole plne -> 2x, ctvrtina -> polovina 
    - agregovaná:
        - zkopírování trvá O(n), k dalšímu dojde nejdřív po n/2 insert nebo n/2 delete 
        - (n0 + k1) + sum_i 2*ki <= n0 + 2k 
    - potencialova metoda: 
        - p: 0 if z poloviny, n z čtvrtiny / plné, zbytek interpolací 
        - přidáváme/odebirame prvek: realT = 1, diffP <= 2, amorT <= 3 
        - pridavame prvek a realokace: amorT = (n+1) + (2-n) = 3
        - odebirame prvek a realokace: amorT = (n+1) + (1-n) = 2

Chování a analýza datových struktur na systémech s paměťovou hierarchií.
- paměťová hierarchie cca:
    - registry: 1kb : 1 cycle 
    - L1-L3; ~10s kB, 100s kB, 1s MB per core; 1s cycles, low 10s, high 10s 
    - RAM: ~low 10GBs, 100s cycles  

    - nejen latence, ale i bandwidth problém: CPU dokáže teoreticky operovat s násobně víc daty než je bandwidth z RAM / main memory

- external memory model: explicitni nacteni do cache vs. cache model: automaticky system (LRU, FIFO, ...)
    - můžeme předpokládat cache model s optimálním controllerem, viz níže LRU je jen konstantně horší (s nutností konstantně větší paměti)

- strategie pro správu cache
    - OPT: Optimální algoritmus, který minimalizuje množství přístupů skrze znalost všeho dopředu 
    - FIFO: smažeme z paměti tu, která je v paměti nejdéle dlouho 
    - LRU: smažeme z paměti tu, která byla nejvíc v minulosti použitá (implementace skrze bidir-spojový seznam, např.)

    - pokud může LRU uložit víc než 2*prvků, pak má LRU nejvýše dvojnásobný počet přenesených bloků proti OPT (+nOPT)
        - fOPT <= nLRU / (nLRU - nOPT) * fOPT + nOPT
        - rozdělíme na bloky s1...s2, v každém má nLRU výpadků -> alespoň nLRU různých bloků -> OPT má nLRU - nOPT výpadků 
        -> fLRU/fOPT <= nLRU/(nLRU-nOPT)
        ~ v posledním kuse dat to platit nemusí, počet výpadků může být menší -> omezený nOPT
        - dvojnásobná cache nezmění asymptotickou analýzu našich alg.


    - pokud musí mít stejně prvků: existuje posloupnost requestů kde tLRU >= (1-e)*C*tOPT   
        - opakuj sekvenci 0..C+1, LRU zmissuje vše, OPT zvládá udržovat C-1 v cachi 
        -> počet opakování podle wanted e

- zjednodušení pro potřeby analýzy
    - pouze 2 úrovně: pomalá, rychlá 
    - cache rozdělená na bloky o velikosti B (prvky velikosti 1) -> počet bloků je P = M/B
    - procesor může přistupovat pouze k datům v cache (~odpovídá realitě)
    - paměť je plně asociativní: každý blok z disku může být kdekoliv v cache
        - velké zjednodušení, cache je většinou n-way-asociativní: set derived adresy, v každém setu plně asociativní velikosti n
        -> určité patterny přístupu do paměti mohou mít velmi nepříjemné kolize 
    - neřešíme koherenci cachí (NUMA, multicore, ...) / writeback/writethrough / ... a další složitosti reálného světa
    - měříme celkový počet přenesených bloků, bez znalosti konkrétních hodnot M, B (cache oblivious analýza)

- dva typy algoritmů: 
    - cache-aware: znají hodnoty M, B a podle nich nastaví své parametry
        - b-stromy, velikost b na B 
    - cache oblivious: je cache efektivní bez znalosti konkrétních hodnot M, B 
        - výhodné pro hierarchii, je optimální pro všechny úrovně cachí 

- přečtení souvislého pole: 
    - ceil(n/B) + 1 // +1 protože nemusíme být zarovnáni 
- binární vyhledávání v setříděném poli 
    - provedeme O(logn) porovnání 
    - posledních O(logB) porovnání je uloženo v nejvýše 2 blocích, zbytek v po 2 různých 
    -> O(logn - logb) = O(log(n/b))

- průchod binární haldou: 
    - stejně jako bin. hledání v poli 

- transpozice matice: naive version
    - nejhorší možný případ: M/B < n 

    - i pokud se řádek vejde do cache, pro transpozici prvního řádku musím načíst první sloupec -> n + 1 bloků 
    - jak začnu číst druhý řádek & druhý sloupec -> načetl jsem mezi tím n jiných řádků -> při LRU pryč z cache 
    -> O(n^2) přístupů do paměti 

- cache-aware alg: 
    - pokud M > 2*B^2, tj. tall cache (víc bloků než velikost jednoho bloku), potřeba v cachi udržet dva bloky najednou 
    - rozdělím matici na BxB bloky a procházím po jednotlivých BxB blocích 
        -> první řádek: čte nejvýše ve 2 blocích, zapisuje do B bloků -> B + 2 bloků -> vejde se 
        -> druhý řádek: zapisuje do stejných bloků, čte z 2 bloků 
        -> O(n^2/B) 
- cache oblivious: 
    - rekurzivně matici rozdělíme na A: A11 A12
                                        A21 A22
    - rekurzivně transponujeme menší a menší podmatice, v jeden moment budou mít velikost ni <= B < 2ni 
        -> nedochází k výpadkům, stejné jako BxB 

- násobení matic: stejně jako transposice 
- merge sort: 
    - potřeba 3 cache bloky, 2*čtení sources, 1*zápis result
    - slytí jedné úrovně: 2n/B + O(1) = O(n/B + 1) přenosů 
    -> celkem O(n/B)*(1+log2n/z) = O(n/B * log(n)+1)
- multiway merge sort: využijeme víc než 3 cache lines at the same time + halda 
    - O(n/B * logn/logK + 1)
    - optimální K = M/2B 

- funnel sort: cache oblivious optimální (za přepdokladu tall cache) verze merge sortu 
    - split input into N^1/3 polí velikosti N^2/3 a sortuj rekurzivně
    - merge sekvence skrze N^1/3 merger, rekurzivně, FIFO queue spojující vždy 2, ...

- binární strom zkrze ukazatele: co uzel to (potenciálně) jiný blok 
- B-strom: pokud uzly velikosti B, tak může být optimálně cache aware 

- van emde boas: 
    - rozsekneme strom v polovině výšky, vzniklé podstromy velikost sqrt(n), do paměti zeshora, zleva ... rekurzivně 
    - v určitý moment se celý podstrom výšky z již vejde do cache: z < log2B < 2z 
    -> počet stromů výšky z na cestě z kořene do listu je h/z <= 2log_B(n) -> O(log_B(n))

- Dynamické stromy: 
    - doplnit dynamický strom T prázdnými uzly &  uložit jako EmbeBoas
    - pro každý uzel definujeme hustotu: poměr reálných a všech uzlů 
        - každá úroveň má horní/dolní práh hustoty, při porušení vyvažujeme 
    - update: O(logB(n) + log^2(n)/B)

