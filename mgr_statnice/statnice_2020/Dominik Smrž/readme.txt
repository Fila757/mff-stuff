Neuronové sítě -- NN pro problémy učení bez učitele (zejména Kohonenovy mapy) [Pilát]
Řekl jsem nějaký krátky obecný úvod, že cílem je najít nějakou strukturu v datech (typycky rozklastrovat). Základní pointa je v tom, že když zpracováváme vstup, najdeme nejbližší neuron, ten zvolíme jako "vítězný" a upravíme jeho váhy (typicky směrem ke vstupu). Takto jednoduchý algoritmus vpodstatě odpovídá k-means. Kohonenovy mapy přidávájí topologii na prosoru neuronů -- není ovlivněn jen vítězný neuron ale i nejbližší neurony v prostoru neuronů. To děláme podle nějaké funkce topologického prostoru (začal jsem muvit o nějakých triviálnách případech té funkce). Nějak jsem tam načrtl adaptační pravidlo, ale ani nevím jestli správně (nějak ho to nezajímalo). Chtěl slyšet "Funkce mexického kloboku", potěšilo ho, že jsem řekl, že je to rozdíl dvou normálních rozdělení. Ukázal jsem "motýla" a jen slovně načrtl, že to nezkonverguje do optima, tím jsme skončili.

Základy složitosti a vyčíslitelnosti -- Aproximační algoritmy a schémata [Petr Kučera]
Zadefinoval jsem co je to Optimalizační problém, a (alfa-)aproximační problém. Uvedl jsem příklad na BinPackingu, kde jsem ukázal 2-aproximační, a ideu důkazu (při dotazu jestli to chce pořádně, řekl, že ne -- ani nevím jestli chctěl tu ideu). Pak jsem zadefinoval PAS a ÚPAS. Řekl jsem, že na Batoh (který po mě chtěli zadefinovat) máme pseudopolynomiální algoritmus s O(nW), kde W je velikost batohu (algortimus jsem nepopisovat, jen řekl, že "si udržujeme tabulku"). Z tohohle algoritmu můžeme odstat ÚPAS, tak že zaokrouhlujeme... tady jsem se zasek, protože mi nic nedávalo smysl. Načež mi Kučera vysvětlil, že tady je právje chyba, že škálovat váhy nemůžeme (při zaokrouhlení dolů nám může vyjít nepřípustné řešení, nahoru zase nedostaneme odhad k optimálnímu řešení). Proto je potřeba vybrat pseudopoly. se složitostí O(nC), kde C je součet cen. Ale to není problém, stačí jen otočit tabulku. V průběhu přišel Čepek a shodli se, že používám divné názvosloví, ale ničemu to nevadí. Celkově odcházel Kučera dost spokojený.

Reprezentace znalostí -- Výroková logika, sémantika, tautologie, dokazatelnost, splnitelnost [Čepek]
Nějak jsem zadefinoval co je to Teorie (množina prvovýroků a formulí = axiomů). V detailech se naštěstí nerýpal -- shodli jsme se na představě že prvovýrok je vlastně proměná, která může být true/false. Model teorie je ohodnocení prvovýroků konzistentní s axiomy dané teorie. Formule je sémanticky dokazatelná, pokud platí ve všech modelech teorie. Sém. splnitelný, pokud platí alespoň v jednom. Tautologie je sémanticky dokazatelná v každé teorii (toto nevím jestli je úplně správně, každopádně se tvářil spokojeně on i Petr Kučera co se přišel podívat). Řekl jsem že známe dokazovací metody jako rezoluce a tablo, které jsou úplné a korektní. Zeptal se mě na ty pojmy "úplné" a "korektní" dokazovací metody, ty jsem zadefinoval. Pak se mě zeptal jak teda funguje ta rezoluce, což jsem mu ukázal na mini-příkladu (nejprve převod do CNF -- co je to CNF -- a pak jeden krok). A pak už jen, že co se stane, když se snažím dokázát spor (dostanu prázdnou klauzuli).

Strojové učení -- Učení s učitelem (hlavně rozhodovací stromy, SVM, ...) [Barták]
Prošli jsme krátký úvod co to je strojové učení atd. Pak jsem jen zmínil lin. regresi a co to je. Pak jsme šli na stromy -- jak funguje inference, jak stavím strom -- snažím se minimalizovat entropii/Gini, to jsem zadefinoval co to je. Pokračuju dokud -- mám data různých cílových tříd/hodnot a různých příznaků a alespoň nějaká data. Co bude značit list, který byl zastaven postupně těmito kritérii (list co byl zastaven, protože tam nejsou ŽÁDNÁ data -- může se stát pro nebinární stromy -- tak rozhoduje inferovanou třídu dle rodiče, jinak jasné). Trošku jsme prošli generalizaci -- můžu penalizovat složité stromy (víc jsem neřekl) nebo dělat pruning kdy na trénovací množině postavím strom a púotom prořezávám dokud to snižuje testovací chybu. Padla otázka proč se teda nezastavím rovnou při buildu -- příklad se XORem. Větou jsem řekl co jsou to náhodné lesy. A pak ve stručnosti (jen ideově) uvedl SVM (hledáme dělící nadrovinu -- trik se zobrazením do složtějšího prostoru, slack; řekl jsem že to je pointa duálního problému kdy nám stačí kernel, a musíme si potom pamatovat jen ty "support vektory" co vyšli se slackem).

Datavé struktury -- Hešování [Majerech]
Majerech za mnou přišel hned jak jsem skončil s Bartákem a zeptal se jeslti můžu nebo se ještě připravuju. Řekl jsem, že jsem si zatím neměl čas to připravit (měl jsem asi jen dvě věty), ale jestli chce, tak to můžu zkusit spatra. Tak řekl ať to zkusím. Takže to bylo dost takové povídání. Řekl jsem motivaci (chceme rychle insert, delete, find). Uvažoval jsem číselné universum, poznamenal, že můžeme hešovat cokoli, ale klidně můžeme uvažovat teď jen čísla. Řekl jsem že uvažujeme nějaké uversum hešovacích fcí a tu vybíráme náhodně. A hešovací fce je fce která prvku z universa přiřadí místo v paměti. Zeptal se jeslti jsem schopný udělat nějakou analýzu na nějakém přístupu jak řešit kolize. Tak jsem začal mluvit o separovaných řetězcích, že délka řetězce, lze při c-univerzálním hešování a m > n odhadnout ve střední hodnotě jedničkou. Tím jsme se dostali k pojmu "faktor zaplnění" (n/m), který chtěl slyšet, a že ho chceme nízký -- udržujeme ho nízký tak, že když se moc veliký tak přehešujeme a hešovací tabulku vytvoříme nanovo dvakrát větší (aby se to uamortizovalo). Pak mi ale řekl, že separované řetězce jsou v praxi příšerný přístup akorát analýza je jednoduchá proto ji všichni dělají, tak jestli neznám něco zajímavějšího. Začal jsem mluvit o kukaččím, což zhodnotil, že je zajímavá volba, ale mám mluvit. Tak jsem ho nějak uvedl, ptal se na jeho výhodu, to jsem nevěděl, tak jsem začal že "ve srovnání s linear probing...", tak jsem byl dotázán abych řekl co to je linear probing. Tady zhruba přišel Antonín Kučera s tím "tak už ho nechej", načež mu Majerech řekl, že "Jemu to už je stejně jedno on už to má jako poslední otázku" a vrátil se ke zkoušení :D No takže jsem řekl co je to linear probing, jak se provádí operace (náhrobky), jeho nevýhody (clustery a teda vetší čas na operace). Pak mi řekl, že to co chtěl slyšet je, že Kukaččí ma garantovaný find (delete) v O(1) -- proto se prý používá, i když je potřeba ho přehešovat relativně často (ptal se při jakém poměru je třeba ho přehešovat, řekl jsem že bych tušil nějak 1/2, řekl že ano, že je to přibližně 0.45, a že ostatní přístupy dovolují i větší faktor zaplnění). Pak jsme řešili rodiny hešovacích funkcí -- tam jsem začal mluvit o ax + b mod p mod m. Načež mi řekl že ano, že o té fci se dá ukázat spoustu fajn teoritických vlastností (univerzálnost) ale, že se v praxi moc nepoužívá protože je časově náročná (modulo je výrazně "dražší" než třeba xor). Tak se ptal jeslti znám nějakou další nebo to je celý můj repertoár. Další jsem moc nevěděl, ale že začal mluvit o xoru tak jsem ze sebe začal soukat tabulační hešování a něco mu o tom řekl. Pak mi ještě chvíli vyprávěl. Celkově velmi příjemné zkoušení a i když jsem sem tam něco nevěděl, tak zkoušející odcházel asi spokojený.